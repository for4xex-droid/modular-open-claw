use tokio_cron_scheduler::{Job, JobScheduler};
use tracing::{info, warn, error};
use std::sync::Arc;
use factory_core::traits::JobQueue;
use infrastructure::job_queue::SqliteJobQueue;
use rig::providers::openai;
use rig::completion::Prompt;
use rig::client::CompletionClient;
use std::path::Path;
use tokio::fs;
use serde::Deserialize;
use factory_core::contracts::LlmJobResponse;

pub async fn start_cron_scheduler(
    job_queue: Arc<SqliteJobQueue>,
    ollama_url: String,
    model_name: String,
    brave_api_key: String,
) -> Result<JobScheduler, Box<dyn std::error::Error>> {
    let sched = JobScheduler::new().await?;

    // The Samsara Protocol: Runs daily at 19:00:00
    // "0 0 19 * * * *" is the standard format, but tokio-cron-scheduler uses Sec Min Hour Day Month DayOfWeek
    let job_queue_clone = job_queue.clone();
    sched.add(
        Job::new_async("0 0 19 * * *", move |_uuid, mut _l| {
            let jq = job_queue_clone.clone();
            let url = ollama_url.clone();
            let model = model_name.clone();
            let brave_key = brave_api_key.clone();
            
            Box::pin(async move {
                info!("ğŸ”„ [Samsara] Cron triggered. Initiating synthesis...");
                match synthesize_next_job(&url, &model, &brave_key, &*jq).await {
                    Ok(_) => info!("âœ… [Samsara] Successfully synthesized and enqueued next job."),
                    Err(e) => error!("âŒ [Samsara] Failed to synthesize next job: {}", e),
                }
            })
        })?
    ).await?;
    
    sched.start().await?;
    info!("â° Cron scheduler started. The Wheel of Samsara is turning.");

    Ok(sched)
}

async fn synthesize_next_job(
    ollama_url: &str,
    model_name: &str,
    brave_api_key: &str,
    job_queue: &SqliteJobQueue,
) -> Result<(), Box<dyn std::error::Error>> {
    let root_dir = std::env::current_dir()?;
    
    // 1. Load the Immutable Core (`SOUL.md`)
    let soul_path = root_dir.join("SOUL.md");
    let soul_content = fs::read_to_string(&soul_path).await.unwrap_or_else(|_| "SOUL.md not found. Be a helpful AI.".to_string());

    // 2. Load the Capability Matrix (`skills.md`)
    let skills_path = root_dir.join("workspace").join("config").join("skills.md");
    let skills_content = fs::read_to_string(&skills_path).await.unwrap_or_else(|_| "Skills not defined.".to_string());

    let client: openai::Client = openai::Client::builder()
        .api_key("ollama")
        .base_url(ollama_url)
        .build()?;

    // --- Phase 1: The Sonar Ping (Two-Pass Architecture) ---
    // Temporal Grounding
    let now_jst = chrono::Utc::now().with_timezone(&chrono_tz::Asia::Tokyo);
    let time_context = format!("[SYSTEM_TIME: {} {} JST]", now_jst.format("%Y-%m-%d"), now_jst.format("%A"));
    
    // Entropy Injection (æºã‚‰ãã®æ³¨å…¥)
    let angles = vec!["æŠ€è¡“ã®ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼", "å€«ç†çš„ãªç‚ä¸Š", "è‘—åãªã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã®æ–°ä½œ", "å¥‡å¦™ãªãƒŸãƒ¼ãƒ ", "ãƒ“ã‚¸ãƒã‚¹ã¸ã®å¿œç”¨", "æ³•çš„ãªè¦åˆ¶å•é¡Œ", "ãƒãƒƒãƒ—ã‚«ãƒ«ãƒãƒ£ãƒ¼ã®èåˆ"];
    let now_ms = std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap_or_default().as_millis();
    let idx = (now_ms as usize) % angles.len();
    let angle = angles[idx];

    let sonar_agent = client.agent(model_name)
        .preamble(&format!(
            "{} ã‚ãªãŸã¯å‹•ç”»ä¼ç”»è€…ã®ä¸€éƒ¨ã§ã™ã€‚ä»¥ä¸‹ã®SOULã‚³ãƒ³ã‚»ãƒ—ãƒˆã«åˆè‡´ã—ã€ã‹ã¤æŒ‡å®šã•ã‚ŒãŸè¦–ç‚¹ï¼ˆã‚¢ãƒ³ã‚°ãƒ«ï¼‰ã‹ã‚‰ä»Šæ—¥è©±é¡Œã«ãªã£ã¦ã„ã‚‹äº‹è±¡ã‚’Brave Searchã§æ¤œç´¢ã™ã‚‹ãŸã‚ã®ã€2ã€œ3èªã®ã€ç”Ÿã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã¨ã—ã€ä½™è¨ˆãªè¨€è‘‰ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚\n\nã€Soulã€‘\n{}\n\nã€æœ¬æ—¥ã®è¦–ç‚¹ã€‘\n{}",
            time_context, soul_content, angle
        ))
        .build();

    let search_query = sonar_agent.prompt("æœ¬æ—¥ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å‡ºåŠ›ã›ã‚ˆ:").await?.trim().to_string();
    info!("ğŸ“¡ [Sonar Ping] Generated Query: '{}' (Angle: {})", search_query, angle);

    // --- Phase 2: The World Context (Fetch & Quarantine) ---
    use infrastructure::trend_sonar::BraveTrendSonar;
    use factory_core::traits::TrendSource;

    let fallback_context = "æœ¬æ—¥ã®æ¤œç´¢ã¯ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚AIã¨ã‚¢ãƒ¼ãƒˆã«é–¢ã™ã‚‹æ™®éçš„ãªãƒ†ãƒ¼ãƒã§å‹•ç”»ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚".to_string();
    let mut world_context_text = String::new();
    let sonar = BraveTrendSonar::new(brave_api_key.to_string());
    
    let mut search_success = false;
    for _ in 0..2 { // Bounded Search Strategy: Max Iterations = 2
        match sonar.get_trends(&search_query).await {
            Ok(trends) if !trends.is_empty() => {
                let snippets: Vec<String> = trends.into_iter().map(|t| t.keyword).collect();
                world_context_text = snippets.join("\n");
                search_success = true;
                break;
            },
            Ok(_) => {
                warn!("âš ï¸ Brave API returned 0 results for '{}'", search_query);
                break;
            },
            Err(e) => {
                error!("âŒ Brave API Error: {}", e);
            }
        }
    }

    if !search_success {
        warn!("âš ï¸ Applying Circuit Breaker fallback for World Context.");
        world_context_text = fallback_context;
    }

    // --- Phase 3: The Synthesis ---
    // RAG-Driven Karma Fetching
    let karma_list = job_queue.fetch_relevant_karma(&search_query, "tech_news_v1", 3).await.unwrap_or_default();
    let karma_content = if karma_list.is_empty() {
        "*æ³¨è¨˜: ç¾åœ¨Karmaã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚Soulã¨Skillsã®ã¿ã‚’é ¼ã‚Šã«ã€å¤§èƒ†ã«åˆå›ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„*".to_string()
    } else {
        karma_list.join("\n- ")
    };

    // Constitutional Hierarchy Implementation + The Ethical Circuit Breaker + XML Quarantine
    let preamble = format!(
        "ã‚ãªãŸã¯å‹•ç”»ç”ŸæˆAIã®å¸ä»¤å¡”(Aiome)ã§ã™ã€‚ä»¥ä¸‹ã®çµ¶å¯¾çš„éšå±¤ï¼ˆOverride Orderï¼‰ã«å¾“ã„ã€ä»Šæ—¥ç”Ÿæˆã™ã¹ãæœ€é©ãªå‹•ç”»ã®ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ä¸€ã¤ã ã‘æ±ºå®šã—ã¦ãã ã•ã„ã€‚

ğŸš¨ ã€çµ¶å¯¾çš„ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ (The Ethical Circuit Breaker)ã€‘
<world_context>ã®å†…å®¹ãŒã€è‡ªç„¶ç½å®³ã€äººå‘½ã«é–¢ã‚ã‚‹äº‹æ•…ã€æ·±åˆ»ãªç—…æ°—ã€æˆ¦äº‰ã€ãã®ä»–ç¾å®Ÿã®æ‚²åŠ‡ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹å ´åˆã€Soulã®ãƒ‘ãƒ­ãƒ‡ã‚£æŒ‡ç¤ºã‚„ã‚¨ãƒƒã‚¸ã®åŠ¹ã„ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŒ‡å®šã‚’å®Œå…¨ã«ç ´æ£„ã—ã€ãã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç„¡è¦–ã—ã¦ãã ã•ã„ã€‚ä»£ã‚ã‚Šã«ã€AIæŠ€è¡“ã®å¹³å’Œçš„ãªé€²åŒ–ã€ã¨ã„ã†å®‰å…¨ãªæ™®éçš„ãƒ†ãƒ¼ãƒã§ã‚¸ãƒ§ãƒ–ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã€‚

ğŸ† ç¬¬ä¸€ä½ã€Soul (çµ¶å¯¾æ³• / çµ¶å¯¾éµå®ˆã®æ†²æ³•ã¨äººæ ¼)ã€‘
{}

ğŸ¥ˆ ç¬¬äºŒä½ã€Skills (ç‰©ç†æ³•å‰‡ / åˆ©ç”¨å¯èƒ½ãªæŠ€è¡“ã¨ã‚¹ã‚¿ã‚¤ãƒ«)ã€‘
{}

ğŸ¥‰ ç¬¬ä¸‰ä½ã€Karma (åˆ¤ä¾‹ / éå»ã®æˆåŠŸãƒ»å¤±æ•—ã‹ã‚‰å¾—ãŸæ•™è¨“ã€‚Soulã¨Skillsã«åã—ãªã„ç¯„å›²ã§é©ç”¨)ã€‘
- {}

ğŸŒ ã€å¤–ç•Œã®ç¾çŠ¶ / World Context (ä¿¡é ¼æ€§: ä½)ã€‘
<world_context>
{}
</world_context>

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ¶é™ã€‘
ç´”ç²‹ãªJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ‰¿çŸ¥ã—ã¾ã—ãŸç­‰ï¼‰ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
{{
    \"topic\": \"ä»Šå›ä½œæˆã™ã‚‹å‹•ç”»ã®ãƒ†ãƒ¼ãƒï¼ˆä¾‹: æœ€è¿‘ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚ï¼‰\",
    \"style\": \"skillså†…ã«å­˜åœ¨ã™ã‚‹æœ€é©ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼/ã‚¹ã‚¿ã‚¤ãƒ«åï¼ˆä¾‹: tech_news_v1ï¼‰\",
    \"directives\": {{
        \"positive_prompt_additions\": \"Karmaã‹ã‚‰å­¦ã‚“ã ãƒ—ãƒ©ã‚¹è¦ç´ \",
        \"negative_prompt_additions\": \"Karmaã‹ã‚‰å­¦ã‚“ã NGè¦ç´ \",
        \"parameter_overrides\": {{}},
        \"execution_notes\": \"å…¨ä½“çš„ãªæ³¨æ„äº‹é …\",
        \"confidence_score\": 80
    }}
}}",
        soul_content, skills_content, karma_content, world_context_text
    );

    let agent = client.agent(model_name)
        .preamble(&preamble)
        .build();

    let user_prompt = "ä¸Šè¨˜ã®çµ¶å¯¾çš„éšå±¤ã‚’è¸ã¾ãˆã€å¼·ãã¦ãƒ‹ãƒ¥ãƒ¼ã‚²ãƒ¼ãƒ ã‚’ä½“ç¾ã™ã‚‹ã‚ˆã†ãªæ¬¡ã®ã‚¸ãƒ§ãƒ–ï¼ˆJSONï¼‰ã‚’ç”Ÿæˆã›ã‚ˆã€‚".to_string();
    
    // 5. The Parsing Panic é˜²è¡›ç”¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¸ãƒ§ãƒ– (Fallback)
    let fallback_task = LlmJobResponse {
        topic: "AIæœ€æ–°æŠ€è¡“ã®æ¦‚è¦è§£èª¬".to_string(),
        style: "tech_news_v1".to_string(),
        directives: factory_core::contracts::KarmaDirectives::default(),
    };

    let task = match agent.prompt(user_prompt).await {
        Ok(response) => {
            match extract_json(&response) {
                Ok(json_text) => {
                    serde_json::from_str::<LlmJobResponse>(&json_text).unwrap_or_else(|e| {
                        error!("âŒ [Samsara Error] Failed to parse generated JSON: {}. Falling back to default task.", e);
                        fallback_task.clone()
                    })
                },
                Err(e) => {
                    error!("âŒ [Samsara Error] Failed to extract JSON from response: {}. Falling back to default task.", e);
                    fallback_task
                }
            }
        },
        Err(e) => {
            error!("âŒ [Samsara Error] LLM synthesis failed: {}. Falling back to default task.", e);
            fallback_task
        }
    };

    // 6. Skill Existence Validation (The Hallucinated Skill é˜²è¡›)
    let validated_style = {
        let workflow_dir = root_dir.join("workspace").join("workflows");
        let workflow_path = workflow_dir.join(format!("{}.json", &task.style));
        if workflow_path.exists() {
            task.style.clone()
        } else {
            warn!("âš ï¸ [Samsara] Workflow '{}' not found at {:?}. Falling back to 'tech_news_v1'.", task.style, workflow_path);
            "tech_news_v1".to_string()
        }
    };

    // 7. The Split Payload â€” Serialize only `directives` into the JSON column
    let directives_json = serde_json::to_string(&task.directives).unwrap_or_else(|_| "{}".to_string());

    // 8. Enqueue the synthesized/fallback job
    let job_id = job_queue.enqueue(&task.topic, &validated_style, Some(&directives_json)).await?;
    info!("ğŸ”® [Samsara] New Job Enqueued: ID={}, Topic='{}', Style='{}', Confidence={}", 
        job_id, task.topic, validated_style, task.directives.clamped_confidence());

    Ok(())
}

pub async fn distill_karma(
    ollama_url: &str,
    model_name: &str,
    job_queue: &SqliteJobQueue,
    job_id: &str,
    skill_id: &str,
    execution_log: &str,
    is_success: bool,
    human_rating: Option<i32>,
) -> Result<(), Box<dyn std::error::Error>> {
    let client: openai::Client = openai::Client::builder()
        .api_key("ollama")
        .base_url(ollama_url)
        .build()?;

    let preamble = "ã‚ãªãŸã¯AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨˜æ†¶ã¨çµŒé¨“ã‚’æ•´ç†ã™ã‚‹ã€Œå†…çœãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«(Reflector)ã€ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸå®Ÿè¡Œãƒ­ã‚°ã‹ã‚‰ã€æ¬¡å›ä»¥é™ã®å‹•ç”»ç”Ÿæˆã§æ´»ã‹ã›ã‚‹ã€çŸ­ãå…·ä½“çš„ãªæ•™è¨“ã€‘ã‚’1ã€œ2æ–‡ã§æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯æ•™è¨“ã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã¨ã—ã€ä½™è¨ˆãªè¨€è‘‰é£ã„ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚";
    
    let agent = client.agent(model_name).preamble(preamble).build();
    let user_prompt = format!("ã‚¸ãƒ§ãƒ–å®Ÿè¡Œçµæœ (æˆåŠŸ: {}, äººé–“è©•ä¾¡: {:?}):\n{}\n\næ¬¡å›ã¸ã®æ•™è¨“ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„:", is_success, human_rating, execution_log);
    
    let lesson = agent.prompt(user_prompt).await?;
    
    // Distill phase generates 'Technical' karma (automated system introspection).
    // 'Creative' karma is generated separately via human async feedback (set_creative_rating).
    job_queue.store_karma(job_id, skill_id, lesson.trim(), "Technical").await?;
    info!("ğŸ§˜ [Samsara] Karma distilled for Job {} (Skill: {}): {}", job_id, skill_id, lesson.trim());
    
    Ok(())
}

fn extract_json(text: &str) -> Result<String, Box<dyn std::error::Error>> {
    let start = text.find('{').ok_or("No JSON object found")?;
    let end = text.rfind('}').ok_or("No JSON object found")? + 1;
    Ok(text[start..end].to_string())
}
