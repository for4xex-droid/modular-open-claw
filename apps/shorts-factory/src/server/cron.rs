use tokio_cron_scheduler::{Job, JobScheduler};
use tracing::{info, error};
use std::sync::Arc;
use factory_core::traits::JobQueue;
use infrastructure::job_queue::SqliteJobQueue;
use rig::providers::openai;
use rig::completion::Prompt;
use rig::client::CompletionClient;
use std::path::Path;
use tokio::fs;
use serde::Deserialize;

#[derive(Deserialize, Clone)]
struct SynthesizedTask {
    topic: String,
    style: String,
    karma_directives: Option<String>,
}

pub async fn start_cron_scheduler(
    job_queue: Arc<SqliteJobQueue>,
    ollama_url: String,
    model_name: String,
) -> Result<JobScheduler, Box<dyn std::error::Error>> {
    let mut sched = JobScheduler::new().await?;

    // The Samsara Protocol: Runs daily at 19:00:00
    // "0 0 19 * * * *" is the standard format, but tokio-cron-scheduler uses Sec Min Hour Day Month DayOfWeek
    let job_queue_clone = job_queue.clone();
    sched.add(
        Job::new_async("0 0 19 * * *", move |_uuid, mut _l| {
            let jq = job_queue_clone.clone();
            let url = ollama_url.clone();
            let model = model_name.clone();
            
            Box::pin(async move {
                info!("üîÑ [Samsara] Cron triggered. Initiating synthesis...");
                match synthesize_next_job(&url, &model, &*jq).await {
                    Ok(_) => info!("‚úÖ [Samsara] Successfully synthesized and enqueued next job."),
                    Err(e) => error!("‚ùå [Samsara] Failed to synthesize next job: {}", e),
                }
            })
        })?
    ).await?;
    
    sched.start().await?;
    info!("‚è∞ Cron scheduler started. The Wheel of Samsara is turning.");

    Ok(sched)
}

async fn synthesize_next_job(
    ollama_url: &str,
    model_name: &str,
    job_queue: &SqliteJobQueue,
) -> Result<(), Box<dyn std::error::Error>> {
    // 1. Load the Immutable Core (`SOUL.md`)
    let root_dir = std::env::current_dir()?;
    let soul_path = root_dir.join("SOUL.md");
    let soul_content = fs::read_to_string(&soul_path).await.unwrap_or_else(|_| "SOUL.md not found. Be a helpful AI.".to_string());

    // 2. Load the Capability Matrix (`skills.md`)
    let skills_path = root_dir.join("workspace").join("config").join("skills.md");
    let skills_content = fs::read_to_string(&skills_path).await.unwrap_or_else(|_| "Skills not defined.".to_string());

    // 3. RAG-Driven Karma Fetching
    let base_topics = vec!["AI", "VTuber", "Cyberpunk", "Philosophical", "Tech Trend"];
    let now = std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap_or_default().as_millis();
    let idx = (now as usize) % base_topics.len();
    let seed_topic = base_topics[idx];
    
    let karma_list = job_queue.fetch_relevant_karma(seed_topic, "tech_news_v1", 3).await.unwrap_or_default();
    
    // Day One Vacuum Handling (Graceful Cold Start)
    let karma_content = if karma_list.is_empty() {
        "*Ê≥®Ë®ò: ÁèæÂú®Karma„ÅØÂ≠òÂú®„Åó„Åæ„Åõ„Çì„ÄÇSoul„Å®Skills„ÅÆ„Åø„ÇíÈ†º„Çä„Å´„ÄÅÂ§ßËÉÜ„Å´ÂàùÂõû„Çø„Çπ„ÇØ„ÇíÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ*".to_string()
    } else {
        karma_list.join("\n- ")
    };

    // 4. Synthesize via LLM
    let client: openai::Client = openai::Client::builder()
        .api_key("ollama")
        .base_url(ollama_url)
        .build()?;

    // Constitutional Hierarchy Implementation
    let preamble = format!(
        "„ÅÇ„Å™„Åü„ÅØÂãïÁîªÁîüÊàêAI„ÅÆÂè∏‰ª§Â°î(Aiome)„Åß„Åô„ÄÇÊú¨Êó•„ÅÆÁô∫ÁÅ´„Ç∑„Éº„Éâ„ÅØ„Äå{}„Äç„Åß„Åô„ÄÇ
‰ª•‰∏ã„ÅÆÁµ∂ÂØæÁöÑÈöéÂ±§ÔºàOverride OrderÔºâ„Å´Âæì„ÅÑ„ÄÅ‰ªäÊó•ÁîüÊàê„Åô„Åπ„ÅçÊúÄÈÅ©„Å™ÂãïÁîª„ÅÆ„Éà„Éî„ÉÉ„ÇØ„Å®„Çπ„Çø„Ç§„É´„Çí‰∏Ä„Å§„Å†„ÅëÊ±∫ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

üèÜ Á¨¨‰∏Ä‰Ωç„ÄêSoul (Áµ∂ÂØæÊ≥ï / Áµ∂ÂØæÈÅµÂÆà„ÅÆÊÜ≤Ê≥ï„Å®‰∫∫Ê†º)„Äë
{}

ü•à Á¨¨‰∫å‰Ωç„ÄêSkills (Áâ©ÁêÜÊ≥ïÂâá / Âà©Áî®ÂèØËÉΩ„Å™ÊäÄË°ì„Å®„Çπ„Çø„Ç§„É´)„Äë
{}

ü•â Á¨¨‰∏â‰Ωç„ÄêKarma (Âà§‰æã / ÈÅéÂéª„ÅÆÊàêÂäü„ÉªÂ§±Êïó„Åã„ÇâÂæó„ÅüÊïôË®ì„ÄÇSoul„Å®Skills„Å´Âèç„Åó„Å™„ÅÑÁØÑÂõ≤„ÅßÈÅ©Áî®)„Äë
- {}

„ÄêÂá∫Âäõ„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂà∂Èôê„Äë
Á¥îÁ≤ã„Å™JSON„ÅÆ„Åø„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ‰ªñ„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàÔºàÊâøÁü•„Åó„Åæ„Åó„ÅüÁ≠âÔºâ„ÅØ‰∏ÄÂàáÂê´„ÇÅ„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ
{{
    \"topic\": \"‰ªäÂõû‰ΩúÊàê„Åô„ÇãÂãïÁîª„ÅÆ„ÉÜ„Éº„ÉûÔºà‰æã: ÊúÄËøë„ÅÆAI„Éã„É•„Éº„Çπ„Åæ„Å®„ÇÅÔºâ\",
    \"style\": \"skillsÂÜÖ„Å´Â≠òÂú®„Åô„ÇãÊúÄÈÅ©„Å™„ÉØ„Éº„ÇØ„Éï„É≠„Éº/„Çπ„Çø„Ç§„É´ÂêçÔºà‰æã: tech_news_v1Ôºâ\",
    \"karma_directives\": \"ÈÅéÂéª„ÅÆÊ•≠(Karma)„Åã„ÇâÂæó„Åü„ÄÅ‰ªäÂõû„ÅÆÁîüÊàê„ÅßÁâπÂà•„Å´ÊÑèË≠ò„Åô„Åπ„ÅçÂÖ∑‰ΩìÁöÑ„Å™„Éó„É≠„É≥„Éó„ÉàËøΩÂä†ÊåáÁ§∫„ÇÑÊ≥®ÊÑèÁÇπÔºà‰æã: '„Éç„Ç™„É≥„Ç´„É©„Éº„ÅØÊéß„Åà„ÇÅ„Å´„Åô„Çã„Åì„Å®'„ÄÇÁâπ„Å´ÊåáÁ§∫„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ nullÔºâ\"
}}",
        seed_topic, soul_content, skills_content, karma_content
    );

    let agent = client.agent(model_name)
        .preamble(&preamble)
        .build();

    let user_prompt = "‰∏äË®ò„ÅÆÁµ∂ÂØæÁöÑÈöéÂ±§„ÇíË∏è„Åæ„Åà„ÄÅÂº∑„Åè„Å¶„Éã„É•„Éº„Ç≤„Éº„É†„Çí‰ΩìÁèæ„Åô„Çã„Çà„ÅÜ„Å™Ê¨°„ÅÆ„Ç∏„Éß„ÉñÔºàJSONÔºâ„ÇíÁîüÊàê„Åõ„Çà„ÄÇ".to_string();
    
    // 5. The Parsing Panic Èò≤Ë°õÁî®„Éá„Éï„Ç©„É´„Éà„Ç∏„Éß„Éñ (Fallback)
    let fallback_task = SynthesizedTask {
        topic: "AIÊúÄÊñ∞ÊäÄË°ì„ÅÆÊ¶ÇË¶ÅËß£Ë™¨".to_string(),
        style: "tech_news_v1".to_string(),
        karma_directives: None,
    };

    let task = match agent.prompt(user_prompt).await {
        Ok(response) => {
            match extract_json(&response) {
                Ok(json_text) => {
                    serde_json::from_str::<SynthesizedTask>(&json_text).unwrap_or_else(|e| {
                        error!("‚ùå [Samsara Error] Failed to parse generated JSON: {}. Falling back to default task.", e);
                        fallback_task.clone()
                    })
                },
                Err(e) => {
                    error!("‚ùå [Samsara Error] Failed to extract JSON from response: {}. Falling back to default task.", e);
                    fallback_task
                }
            }
        },
        Err(e) => {
            error!("‚ùå [Samsara Error] LLM synthesis failed: {}. Falling back to default task.", e);
            fallback_task
        }
    };

    // 6. Enqueue the synthesized/fallback job
    let job_id = job_queue.enqueue(&task.topic, &task.style, task.karma_directives.as_deref()).await?;
    info!("üîÆ [Samsara] New Job Enqueued: ID={}, Topic='{}', Style='{}', Directives='{:?}'", job_id, task.topic, task.style, task.karma_directives);

    Ok(())
}

pub async fn distill_karma(
    ollama_url: &str,
    model_name: &str,
    job_queue: &SqliteJobQueue,
    job_id: &str,
    skill_id: &str,
    execution_log: &str,
    is_success: bool,
    human_rating: Option<i32>,
) -> Result<(), Box<dyn std::error::Error>> {
    let client: openai::Client = openai::Client::builder()
        .api_key("ollama")
        .base_url(ollama_url)
        .build()?;

    let preamble = "„ÅÇ„Å™„Åü„ÅØAI„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÅÆË®òÊÜ∂„Å®ÁµåÈ®ì„ÇíÊï¥ÁêÜ„Åô„Çã„ÄåÂÜÖÁúÅ„É¢„Ç∏„É•„Éº„É´(Reflector)„Äç„Åß„Åô„ÄÇ‰∏é„Åà„Çâ„Çå„ÅüÂÆüË°å„É≠„Ç∞„Åã„Çâ„ÄÅÊ¨°Âõû‰ª•Èôç„ÅÆÂãïÁîªÁîüÊàê„ÅßÊ¥ª„Åã„Åõ„Çã„ÄêÁü≠„ÅèÂÖ∑‰ΩìÁöÑ„Å™ÊïôË®ì„Äë„Çí1„Äú2Êñá„ÅßÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÂá∫Âäõ„ÅØÊïôË®ì„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„Åø„Å®„Åó„ÄÅ‰ΩôË®à„Å™Ë®ÄËëâÈÅ£„ÅÑ„ÅØÂê´„ÇÅ„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ";
    
    let agent = client.agent(model_name).preamble(preamble).build();
    let user_prompt = format!("„Ç∏„Éß„ÉñÂÆüË°åÁµêÊûú (ÊàêÂäü: {}, ‰∫∫ÈñìË©ï‰æ°: {:?}):\n{}\n\nÊ¨°Âõû„Å∏„ÅÆÊïôË®ì„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ:", is_success, human_rating, execution_log);
    
    let lesson = agent.prompt(user_prompt).await?;
    
    // Distill phase inherently binds the karma to the specific skill_id used.
    job_queue.store_karma(job_id, skill_id, lesson.trim(), is_success, human_rating).await?;
    info!("üßò [Samsara] Karma distilled for Job {} (Skill: {}): {}", job_id, skill_id, lesson.trim());
    
    Ok(())
}

fn extract_json(text: &str) -> Result<String, Box<dyn std::error::Error>> {
    let start = text.find('{').ok_or("No JSON object found")?;
    let end = text.rfind('}').ok_or("No JSON object found")? + 1;
    Ok(text[start..end].to_string())
}
