use factory_core::contracts::{ConceptRequest, ConceptResponse};
use factory_core::traits::AgentAct;
use factory_core::error::FactoryError;
use async_trait::async_trait;
use rig::providers::openai;
use rig::client::CompletionClient;
use rig::completion::Prompt;
use tracing::{info, warn, error};

/// å‹•ç”»ã‚³ãƒ³ã‚»ãƒ—ãƒˆç”Ÿæˆæ©Ÿ (Director)
/// 
/// ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€LLMã‚’ä½¿ç”¨ã—ã¦
/// å…·ä½“çš„ãªå‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã€è„šæœ¬ã€ç”»åƒç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚
pub struct ConceptManager {
    url: String,
    model: String,
}

impl ConceptManager {
    pub fn new(api_base: &str, model: &str) -> Self {
        Self {
            url: api_base.to_string(),
            model: model.to_string(),
        }
    }

    fn get_client(&self) -> Result<openai::Client, FactoryError> {
        openai::Client::builder()
            .api_key("ollama")
            .base_url(&self.url)
            .build()
            .map_err(|e| FactoryError::Infrastructure { reason: format!("Failed to build LLM client: {}", e) })
    }
}

#[async_trait]
impl AgentAct for ConceptManager {
    type Input = ConceptRequest;
    type Output = ConceptResponse;

    async fn execute(
        &self,
        input: Self::Input,
        _jail: &bastion::fs_guard::Jail,
    ) -> Result<Self::Output, FactoryError> {
        info!("ğŸ¬ ConceptManager: Generating video concept from {} trends...", input.trend_items.len());

        let client = self.get_client()?;
        let agent = client.agent(&self.model)
            .preamble("ã‚ãªãŸã¯ YouTube Shorts ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªå‹•ç”»ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼ã§ã™ã€‚
            ä¸ãˆã‚‰ã‚ŒãŸãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ãã€è¦–è´è€…ã®ç›®ã‚’å¼•ãå‹•ç”»ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’1ã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚
            
            ä»¥ä¸‹ã®æ¡ä»¶ï¼ˆ3å¹•æ§‹æˆãƒ»æ§‹é€ åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã‚’å³å®ˆã—ã¦ãã ã•ã„ï¼š
            1. å‡ºåŠ›ã¯ç´”ç²‹ãª JSON ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ã¿ã¨ã—ã€ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ãªã„ã€‚
            2. JSON ã¯ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤ã“ã¨ï¼š
               - 'title': å‹•ç”»ã®ã‚¿ã‚¤ãƒˆãƒ« (æ—¥æœ¬èª)
               - 'script_intro': å°å…¥éƒ¨ï¼ˆ3ã€œ5ç§’ï¼‰ã®è„šæœ¬ (æ—¥æœ¬èª)
               - 'script_body': æœ¬ç·¨ï¼ˆ15ã€œ45ç§’ï¼‰ã®è„šæœ¬ (æ—¥æœ¬èª)
               - 'script_outro': çµæœ«ãƒ»ã‚ªãƒï¼ˆ5ã€œ10ç§’ï¼‰ã®è„šæœ¬ (æ—¥æœ¬èª)
               - 'common_style': å…¨ã‚·ãƒ¼ãƒ³å…±é€šã®ç”»é¢¨ã€ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ã€ç‰¹å®šã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŒ‡å®š (è‹±èª)
               - 'visual_prompts': å°å…¥ã€æœ¬ç·¨ã€çµæœ«ã®å„ã‚·ãƒ¼ãƒ³ã«å¯¾å¿œã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚„èƒŒæ™¯æå†™ï¼ˆè‹±èªã€å¿…ãš3ä»¶ï¼‰
               - 'metadata': ãã®ä»–ã®è¨­å®š (HashMap<String, String>)
            3. è¦–è´ç¶­æŒç‡ã‚’é«˜ã‚ã‚‹ãŸã‚ã€å„ãƒ‘ãƒ¼ãƒˆã¯èµ·æ‰¿è»¢çµã‚’æ„è­˜ã—ã€è¦–è¦šçš„ãªå¤‰åŒ–ãŒä¼ã‚ã‚‹ã‚ˆã†ã«æå†™ã—ã¦ãã ã•ã„ã€‚")
            .build();

        let trend_list = input.trend_items.iter()
            .map(|i| format!("- {} (Score: {})", i.keyword, i.score))
            .collect::<Vec<_>>()
            .join("\n");

        let user_prompt = format!("ãƒˆãƒ¬ãƒ³ãƒ‰ãƒªã‚¹ãƒˆï¼š\n{}\n\nå‹•ç”»ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚", trend_list);

        let result = match agent.prompt(user_prompt).await {
            Ok(response) => {
                // JSON ã®ã¿ã‚’æŠ½å‡º
                let json_text = extract_json(&response)?;
                
                let concept: ConceptResponse = serde_json::from_str(&json_text)
                    .map_err(|e| {
                        error!("Failed to parse LLM response as JSON: {}. Response: {}", e, json_text);
                        FactoryError::Infrastructure { reason: format!("LLM JSON Parse Error: {}", e) }
                    })?;

                info!("âœ… ConceptManager: Concept generated: '{}'", concept.title);
                Ok(concept)
            }
            Err(e) => {
                error!("LLM Error: {}", e);
                Err(FactoryError::Infrastructure { reason: format!("LLM Prompt Error: {}", e) })
            }
        };

        // VRAM è§£æ”¾ãƒ—ãƒ­ãƒˆã‚³ãƒ« (keep_alive: 0)
        // rig-core ã®èƒŒå¾Œã«ã‚ã‚‹ Ollama ã«ç›´æ¥ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’æŒ‡ç¤º
        if let Err(e) = self.unload_model().await {
            warn!("âš ï¸ ConceptManager: Failed to unload model: {}", e);
        }

        result
    }
}

impl ConceptManager {
    /// Ollama ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å³æ™‚ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€VRAM ã‚’è§£æ”¾ã™ã‚‹
    async fn unload_model(&self) -> Result<(), Box<dyn std::error::Error>> {
        info!("ğŸ§¹ ConceptManager: Releasing VRAM (keep_alive: 0)...");
        let client = reqwest::Client::new();
        let body = serde_json::json!({
            "model": self.model,
            "keep_alive": 0
        });

        // /v1/chat/completions ã§ã¯ãªãã€Ollama è‡ªä½“ã® /api/generate ã‚’å©ãå¿…è¦ãŒã‚ã‚‹å ´åˆãŒå¤šã„
        // api_base ãŒ http://.../v1 ã®å ´åˆã¯ã€/v1 ã‚’é™¤ã„ãŸãƒ™ãƒ¼ã‚¹URLã‚’å–å¾—
        let base_url = self.url.trim_end_matches("/v1");
        let unload_url = format!("{}/api/generate", base_url);

        client.post(unload_url)
            .json(&body)
            .send()
            .await?;

        Ok(())
    }
}

/// æ–‡å­—åˆ—ã‹ã‚‰JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã—ã¦æŠ½å‡ºã™ã‚‹
fn extract_json(text: &str) -> Result<String, FactoryError> {
    if let (Some(start), Some(end)) = (text.find('{'), text.rfind('}')) {
        Ok(text[start..=end].to_string())
    } else {
        Err(FactoryError::Infrastructure { reason: "LLM response did not contain JSON".into() })
    }
}
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_extract_json_block() {
        let text = "Here is the result: {\"title\": \"test\"} Hope you like it.";
        let result = extract_json(text).unwrap();
        assert_eq!(result, "{\"title\": \"test\"}");
    }

    #[test]
    fn test_extract_json_no_block() {
        let text = "There is no json here";
        let result = extract_json(text);
        assert!(result.is_err());
    }
}
